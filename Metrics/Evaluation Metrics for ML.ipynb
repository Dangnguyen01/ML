{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Abstract**\n",
    "Trong quá trình xây dựng một mô hình Machine Learning, một phần không thể thiếu để xem xét mô hình có chất lượng tốt hay không chính là đánh giá mô hình. Đánh giá mô hình giúp chúng ta chọn lựa được các mô hình phù hợp với bài toán cụ thể. Để có thể áp dụng đúng thước đo đánh giá mô hình phù hợp, chúng ta cần hiểu bản chất, ý nghĩa cũng như các trường hợp sử dụng nó. Cùng phân tích và tìm hiểu các thước đo này nhé!\n",
    "Bài viết sẽ tập trung phân tích các metric đánh giá đối với: Mô hình phân loại (`Classification`), mô hình hồi quy (`regression`) và xếp hạng (`Ranking`)\n",
    "\n",
    "#### **Bài toán phân loại (Classification)**\n",
    "Classification là một bài toán được sử dụng vô cùng rộng rãi trong Machine Learning với các tính ứng dụng đa dạng như nhận diện khuôn mặt, phân loại video Youtube, phân loại văn bản, phân loại giọng nói,...\n",
    "Có thể kể tới một vài mô hình tiêu biểu như Support Vector Machine (SVM), Logistic regresstion, Decision Trees, Random Forest, XGboost,... Dưới đây là một số metrics để đánh giá mô hình phân loại\n",
    "\n",
    "##### Confusion Matrix (Đây không phải là một metric, nhưng rất quan trọng)\n",
    "Chúng ta cùng tìm hiểu một thuật ngữ cơ bản được sử dụng trong các bài toán phân loại - Confusion Matrix (AKA error matrix). Nó thể hiện được có bao nhiêu điểm dữ liệu thực sự thuộc vào một class, và được dự đoán là rơi vào một class. Để dễ hiểu, chúng ta cùng làm một ví dụ.\n",
    "\n",
    "Ví dụ một bài toán phân loại ảnh đó là mèo hay không, trong dữ liệu đoán có 100 ảnh là con mèo, 1000 ảnh không phải con mèo. Ở đây, kết quả dự đoán là như sau:\n",
    "\n",
    "Trong 100 ảnh mèo dự đoán đúng 90 ảnh, còn 10 ảnh được dự đoán là không phải. Nếu ta coi `cat` là *`positive`* và `non-cat` afl *`negative`*, thì 90 ảnh được dự đoán là `cat`, được gọi là **True Positive**, còn 10 ảnh được dự đoán là `non-cat` kia được gọi là **False Negative**\n",
    "\n",
    "Để dễ nhớ, ta lưu ý như sau\n",
    "* True/False chỉ những gì ta đã dự đoán là đúng hay chưa\n",
    "* Positive/Nagative chỉ những gì ta dự đoán (có hoặc không). Nói cách khác, nếu thay chữ True tức là dự đoán đúng (là cat hay non-cat, chỉ cần đúng), còn False thì ngược lại."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Classication Accuracy**\n",
    "Đây là độ đo của bài toán phân loại mà đơn giản nhất, tính toán bằng cách lấy số dự đoán đúng chia cho toàn bộ các dự đoán. Ví dụ bài toán Cat/Non-cat như trên, độ chính xác sẽ được tính như sau:\n",
    "\n",
    "$$Classification\\,Accuracy=(90+940)/(1000+100)=93,6\\%$$\n",
    "Nhược điểm của cách đánh giá này là chỉ cho biết được bao nhiêu phần trắm lượng dữ liệu được phân loại đúng mà không chỉ ra được cụ thể mỗi loại được phân loại như thế nào, lớp nào được phân loại đúng nhiều hay dữ liệu của lớp nào thường bị phân loại nhầm nhất vào các lớp khác"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Precision**\n",
    "Như đã nói ở trên, ta sẽ có rất nhiều trường hợp thước đo Accuracy không phản ảnh đúng hiệu quả của mô hình. Giả sử mô hình dự đoán tất cả 1100 ảnh là Non-cat, thì Accuracy vẫn đạt tới $1000/1100 =90.9%$, khá cao nhưng thực chất mô hình khá tồi. Vì vậy chúng ta cần một metric khác có thể khắc phục yếu điểm này. Precision là một trong những metrics có thể khắc phục được, công thức như sau:\n",
    "$$Precision = \\frac{True\\,Positive}{True\\,Positive+False\\,Positive}$$\n",
    "\n",
    "Áp dụng vào bài toán Cat/Non-cat, Precision sẽ được tính như sau:\n",
    "$$Precision(cat) = \\frac{90}{90+60}=60\\%$$\n",
    "$$Precision(non-cat)=\\frac{940}{940+10}=98.9\\%$$\n",
    "\n",
    "Có thể thấy việc dự đoán Cat chưa thực sự tốt nhờ phép đo Precision này. Precision sẽ cho chúng ta biết thực sự có bao nhiêu dự đoán Positive là thật sự True."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Recall**\n",
    "Recall cũng là một metric quan trọng, nó đo lượng tỉ lệ dự báo chính  xác các trường hợp positive trên toàn bộ các mẫu thuộc nhóm Positive. Công thức của Recall như sau:\n",
    "$$Recall = \\frac{True\\,Positive}{True\\,Positive+False\\,Negative}$$\n",
    "Áp dụng vào bài toán Cat/Non-cat, Precision sẽ được tính như sau:\n",
    "$$Recall(cat)=\\frac{90}{90+10}=90\\%$$\n",
    "$$Recall(non-cat)=\\frac{940}{940+60}=94\\%$$\n",
    "\n",
    "Recall cao đồng nghĩa với việc True Positive Rate cao, tức là tỉ lệ bỏ sót các điểm thực sự là positive thấp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **F1-score**\n",
    "Tùy thuộc vào bài toán mà sẽ muốn ưu tiên sử dụng Recall hay Precision. Nhưng cũng có rất nhiều bài toán mà cả hai metrics này đều quan trọng. Một metric phổ biến đã kết hợp cả Recall và Precision lại được gọi là F1-score\n",
    "\n",
    "F1-score được tính theo công thức sau:\n",
    "$$F1-score=\\frac{2*Precision*Recall}{Precision+Recall}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Sensivity-Specificity**\n",
    "\n",
    "Sensivity và Specificity là 2 metrics được sử dụng trong các bài toán phân loại liên quan đến y học và sinh học. Chúng được định nghĩa như sau:\n",
    "$$Sensivity = Recall=\\frac{True\\,Positive}{True\\,Positive+False\\,Negative}$$\n",
    "$$Specificity = True\\,Nagative\\, Rate=\\frac{True\\,Negative}{True\\,Negative+False\\,Positive}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **AUC**\n",
    "AUC (Area Under the Curve) là một phép đo tổng hợp về hiệu suất của phân loại nhị phân trên tất cả các giá trị ngưỡng có thể có. Để hiểu rõ về metric này, chúng ta sẽ tìm hiểu một khái niệm cơ sở trước, đó là ROC curve\n",
    "\n",
    "ROC curve (The receiver operating characteristic curve) là một đường cong biểu diễn hiệu suất phân loại một mô hình phân loại tại các ngưỡng threshold. Về cơ bản, nó hiển thị True Positive Rate (TPR) so với False Positive Rate (FPR) đối với các giá trị ngưỡng khác nhau. Các giá trị TPR, FPR được tính như sau:\n",
    "\n",
    "$$TPR = \\frac{True\\,Positive}{True\\,Positive+False\\,Negative}$$\n",
    "$$FPR = \\frac{False\\,Positive}{False\\,Positive+True\\,Negative}$$\n",
    "\n",
    "Cùng làm một ví dụ để dễ hình dung\n",
    "\n",
    "Có rất nhiều mô hình phân loại mang tính xác suất, ví dụ dự đoán xác suất của một mẫu là Cat. Chúng so sánh xác suất đầu ra với một số ngưỡng giới hạn và nếu nó lớn hơn ngưỡng đó, mô hình dự đoán là nhãn Cat, còn không thì là Non-cat\n",
    "\n",
    "Ví dụ mô hình của bạn dự đoán giá trị xác suất cho 4 samples lần lượt là $[0.45,0.6,0.7,0.3]$. Tùy vào giá trị ngưỡng mà sẽ có các nhãn đầu ra dự đoán khác nhau:\n",
    "* Ngưỡng là 0.5: Sample 2, 3 là Cat\n",
    "* Ngưỡng là 0.25: Tất cả Samples đều là Cat\n",
    "* Ngưỡng là 0.8: Tất cả Samples là Non-cat\n",
    "\n",
    "Có thể thấy với các ngưỡng khác nhau, chúng ta sẽ có kết quả dự đoán khác nhau, kéo theo các giá trị như Precision hay Recall cũng sẽ khác nhau\n",
    "\n",
    "ROC tìm ra TPR và FPR ứng với các giá trị ngưỡng khác nhau và vẽ biểu đồ để dễ quan sát TPR so với FPR. Ví dụ dưới đây là một đường cong ROC\n",
    "\n",
    "<img src = 'roc-curve-v2.png'>\n",
    "\n",
    "AUC là chỉ số được tính toán dựa trên đường cong ROC nhằm đánh giá khả năng phân loại của mô hình tốt như thế nào. Phần diện tích nằm dưới đường cong ROC và trên trục hoành chính là AUC, có giá trị nằm trong khoảng [0,1]\n",
    "\n",
    "<img src = 'Basic_AUC_annotated.png'>\n",
    "\n",
    "Khi diện tích này càng lớn, đường cong này sẽ dần tiệm cận với đường thẳng $y=1$ tương đương với khả năng phân loại của mô hình càng tốt. Còn khi đường cong ROC nằm sát với đường chéo qua hai điểm $(0,0)$ và $(1,1)$, mô hình sẽ tương đương với một phân loại ngẫu nhiên\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bài toán hồi quy (Regression)**\n",
    "\n",
    "Mô hình hồi quy (Regression Model) được sử dụng để dự đoán các giá trị mục tiêu là giá trị liên tục. Mô hình này cũng có tính ứng dụng vô cùng rộng, từ bài toán dự đoán giá nhà, hệ thống định giá thương mại điện tử, dự báo thời tiết, dự đoán thị trường chứng khoán, cho đến chuyển hóa độ phân giải hình ảnh siêu cao, tình năng học tập thông qua bộ mã hóa tự động, nén hình ảnh\n",
    "\n",
    "Một vài mô hình hồi quy phổ biến có thể kể tới như hồi quy tuyền tính (Linear Regression), Random Forest, Convolution neural network (tùy vào bài toán mà CNN sẽ phục vụ, CNN có thể đáp ứng cả bài toán phân loại cũng như hồi quy),..\n",
    "\n",
    "Các metrics được sử dụng để đánh giá mô hình hồi quy phải có khả năng làm việc với tập các giá trị liên tục, một số metrics phổ biến như sau:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MSE**\n",
    "\n",
    "MSE (Mean Square Error) có lẽ là một metric phổ biến nhất trong các bài toán hồi quy. Về cơ bản, nó tính trung bình của bình phương sai số giữa giá trị thực tế và giá trị dự đoán.\n",
    "\n",
    "Giả sử ta có một bài toán mà chắc hẳn ai đọc về Machine Learning cũng từng đọc qua, chính là bài toán dự đoán giá nhà. Coi giá trị thực tế thứ $i$ là $y_i$, còn giá trị dự đoán của căn nhà đó là $y_i^{'}$. Vậy MSE có thể được tính như sau:\n",
    "\n",
    "$$MSE=\\frac{1}{N}\\sum_{i=1}^{n}(y_i-y_i^{'})^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MAE**\n",
    "\n",
    "MAE (Mean Absolute Error) là 1 metric đánh giá mô hình bằng cách tính trung bình giá trị tuyệt đối sai số giữa giá trị thực tế và giá trị dự đoán. Công thức MAE được định nghĩa như sau:\n",
    "\n",
    "$$MAE=\\frac{1}{N}\\sum_{i=1}^{n}|y_i-y_i^{'}|$$\n",
    "\n",
    "MSE được biết đến là mạnh mẽ hơn đối với các yếu tố ngoại lai (outliers) so với MAE. Lý do chính là bởi MSE sử dụng bình phương lỗi, các ngoại lai (những samples mà có lỗi cao hơn hẳn so với các samples khác) sẽ được chú ý và chiếm ưu thế hơn (do tính bình phương) trong việc đánh giá và điều này tác động đến các tham số của mô hình"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bài toán xếp hạng (Ranking)**\n",
    "\n",
    "Ranking được coi là một vấn đề cơ bản trong Machine Learning, nó xếp hạng một danh sách các mục dựa vào sự liên quan giữa chúng trong các bài toán cụ thể (ví dụ như xếp hạng các pages trên Google dựa vào sự liên quan với câu truy vấn tìm kiếm). Theo mình tìm hiểu được, Ranking được ứng dụng rộng rãi trong thương mại điện tử (E-commerce) và các công cụ tìm kiếm (search engines), cụ thể:\n",
    "* Gợi ý phim ảnh (Netflix, Youtube)\n",
    "* Xếp hạng page của Google\n",
    "* Xếp hạng sản phẩm thương mại điện tử (Amazon)\n",
    "* Tự động hoàn thiện câu truy vấn\n",
    "* Tìm kiếm hình ảnh (vimeo)\n",
    "* Tìm kiếm nhà nghỉ (Expedia/Booking)\n",
    "\n",
    "Trong bài toán Ranking, mô hình cố gắng dự đoán thứ hạng (hoặc chỉ số liên quan) của một danh sách các mục đối với task cụ thể. Thuật toán đối với Ranking có thể chia làm các nhóm sau:\n",
    "\n",
    "* Point-wise models: Dự đoán một điểm số đối với từng cặp truy vấn-văn bản trong dataset, và sử dụng nó để xếp hạng các mục\n",
    "* Pair-wise models: Học một phân loại nhị phân mà có thể trả lời rằng văn bản này có liên quan tới truy vấn này hay không?\n",
    "* List-wise models: Tối ưu hóa trực tiếp giá trị của một trong các thước đo đánh giá, được tính trung bình trên tất cả các truy vấn\n",
    "\n",
    "Trong quá trình đánh giá, dự trên thứ tự thực của danh sách các mục cho một số truy vấn, chúng ta muốn biết việc dự đoán các mục đó tốt như thế nào. Có khá nhiều metrics được đề xuất như MRR, Precision@K, DCG&NDCG, MAP, Kendall’s tau, … tuy nhiên mình sẽ tập trung vào 3 metrics sau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MRR**\n",
    "\n",
    "Mean Reciprocal Rank (MRR) là một trong những metrics đơn giản nhất trong việc đánh giá các ranking models. MRR tính trung bình của các thức hạng tương ứng của mục liên quan đầu tiên đối với tập các truy vấn Q, có thể địn nghĩa nó như sau\n",
    "\n",
    "$$MRR=\\frac{1}{|Q|}\\sum_{i=1}^{|Q|}\\frac{1}{rank_i}$$\n",
    "\n",
    "Ví dụ, ta có bảng sau, tương ứng với các queries 1,2,3 sẽ có danh sách dự đoán và đáp án đúng\n",
    "\n",
    "<img src='table.png'>\n",
    "\n",
    "Vậy\n",
    "\n",
    "$$MRR = \\frac{1}{3}*(\\frac{1}{2}+\\frac{1}{1}+\\frac{1}{3})=\\frac{11}{18}$$\n",
    "\n",
    "Một trong những hạn chế của MRR là nó chỉ tính đến thứ hạng của một trong các mục (mục có liên quan nhất, như ở query 2, chỉ quan tâm tới dự đoán d đầu tiên) và bỏ qua mục khác."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Precision at K**\n",
    "\n",
    "Precision@k : số tài liệu thật sự liên quan đến truy vấn trong k tài liệu có dự đoán liên quan cao nhất\n",
    "\n",
    "$$Presicion@k=\\frac{Number\\,of\\,relevant\\,document\\,among\\,the\\,top\\,k\\,ocuments}{k}$$\n",
    "\n",
    "Để mình ví dụ hơn cho dễ hiểu nhé, bạn tìm kiếm từ khóa “Phim Mỹ”, và trong trang đầu tiến, có 8 trên 10 phim gợi ý bạn là phim Mỹ, vậy Precision@10 đối với truy vấn này là 8/10 = 0.8\n",
    "\n",
    "Khái quát hóa, để tính Precision@k của tập các truy vấn Q, bạn có thể tính bằng cách lấy trung bình của các giá trị Precision@k của các queries trong Q\n",
    "\n",
    "Hạn chế của Precision@k đó là nó không tốt đối với việc tính đến vị trị các tài liệu liên quan bởi nó chỉ tính số lượng"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DCG-NDCG**\n",
    "\n",
    "Normalized Discounted Cumulative Gain (NDCG) có lẽ là metric được được dung phổ biến nhất trong các bài toán learning to rank. Trái ngược với các metrics trước đó, NDCG xem xét thứ tự và sự liên quan quan trọng của các tài liệu, đồng thời chú trọng việc đưa ra các tài liệu có liên quan cao và danh sách được đề xuất\n",
    "\n",
    "Nghe khá là khó hiểu nhỉ? Trước khi tìm hiểu kĩ hơn về NDCG, cùng nhau tìm hiểu 2 metrics liên quan là Cumulative Gain (CG) và Discounted Cumulative Gain (DCG) nào!\n",
    "\n",
    "Cumulative Gain (CG) của một tập các tài liệu được truy xuất là tổng các điểm liên quan (relevance score) của chúng đối với câu truy vấn, được định nghĩa như sau:\n",
    "\n",
    "$$CG_p=\\sum_{i=1}^prel_i$$\n",
    "\n",
    "Discounted Cumulative Gain (DCG) là phiên bản có trọng số của CG, sử dụng logarit để giảm relevance score tương ứng với vị trí của các kết quả. Điều này hữu ích với việc muốn ưu tiên cao hơn cho một vài mục tiêu đầu tiến sau khi phân tích hiệu suất của một hệ thống\n",
    "\n",
    "$$DCG_p=\\sum^p_{i=1}\\frac{rel_i}{log_2(i+1)}$$\n",
    "\n",
    "DCG dựa trên giả định sau:\n",
    "\n",
    "Các tài liệu có liên quan cao sẽ hưu ích hơn nếu xuất hiện sớm hơn trong kết quả tìm kiếm\n",
    "Các tài liệu có liên quan cao sẽ hữu ích hơn các tài liệu có liên quan bên lề tốt hơn các tài liệu không liên quan\n",
    "Normalized Discounted Cumulative Gain (NDCG) cố gắng nâng cao DCG để phù hợp hơn với các ứng dụng thực tế. Bởi tập hợp các mục được truy xuất có thể khác nhau về kích thước giữa các truy vấn hay hệ thống, NDCG cố gắng so sánh hiệu suất bằng các sử dụng phiên bản chuẩn hóa của DCG. Nói cách khác, nó sắp xếp các tài liệu của 1 danh sách kết quả theo mức độ liên quan, tìm vị trí p có DCG cao nhất, và sử dụng để chuẩn hóa DCG như sau:\n",
    "\n",
    "$$nDCG_p=\\frac{DCG_p}{IDCG_p}$$\n",
    "\n",
    "Trong đó, IDCG (Ideal Discounted Cumulative Gain), được định nghĩa như sau:\n",
    "\n",
    "$$IDCG_p=\\sum_{i=1}^{|REL_p|}\\frac{2^{rel_i}-1}{log_2(i+1)}$$\n",
    "\n",
    "NDCG là một metric khá phổ biến, tuy nhiên cũng có một số hạn chế nhất định. Một trong những hạn chế chính của nó là nó không bắt được các “bad documents” trong kết quả. Nó có thể không phù hợp để đo lường hiệu xuất của các truy vấn mà thường có một số kết quả tốt ngang nhau."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
